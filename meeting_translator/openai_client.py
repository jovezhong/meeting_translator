"""
OpenAI Realtime API Client
Implements real-time audio translation using OpenAI's Realtime API

S2T Mode (audio_enabled=False): Uses streaming transcription API (intent=transcription)
          with gpt-4o-transcribe + separate GPT translation for accurate results
          without conversation interference

S2S Mode (audio_enabled=True): Uses conversation API for audio-to-audio translation
"""

import os
import re
import base64
import asyncio
import json
import audioop
import websockets
import time
from typing import Dict, Optional
try:
    import pyaudiowpatch as pyaudio
except ImportError:
    import pyaudio

# å¯¼å…¥åŸºç¡€ç±»ï¼ˆå·²åŒ…å« OutputMixinï¼‰
from translation_client_base import BaseTranslationClient, TranslationProvider
# å¯¼å…¥ç»Ÿä¸€çš„è¾“å‡ºç®¡ç†å™¨
from output_manager import Out

try:
    from python_socks.async_.asyncio import Proxy
    PROXY_AVAILABLE = True
except ImportError:
    PROXY_AVAILABLE = False

# OpenAI client for GPT translation (separate from WebSocket)
try:
    from openai import OpenAI
    OPENAI_SDK_AVAILABLE = True
except ImportError:
    OPENAI_SDK_AVAILABLE = False


class OpenAIClient(BaseTranslationClient):
    """
    OpenAI Realtime API å®¢æˆ·ç«¯

    æ”¯æŒ S2S å’Œ S2T ä¸¤ç§æ¨¡å¼ï¼š
    - S2S (audio_enabled=True): è¯­éŸ³è¾“å…¥ â†’ Realtime API â†’ è¯­éŸ³è¾“å‡º
    - S2T (audio_enabled=False): è¯­éŸ³è¾“å…¥ â†’ gpt-4o-mini-transcribe-2025-12-15 (streaming ASR) â†’ GPTç¿»è¯‘ â†’ æ–‡æœ¬è¾“å‡º

    S2T æ¨¡å¼ä½¿ç”¨ä¸¤é˜¶æ®µå¤„ç†ï¼š
    1. gpt-4o-mini-transcribe-2025-12-15: æµå¼è¯­éŸ³è¯†åˆ«ï¼ˆçº¯ASRï¼Œæ— å¯¹è¯ï¼‰
       - 90% fewer hallucinations vs Whisper v2
       - 70% fewer hallucinations vs previous gpt-4o-transcribe
       - Optimized for real-world conversational settings
    2. GPT-4o-mini: æ–‡æœ¬ç¿»è¯‘ï¼ˆé«˜è´¨é‡ç¿»è¯‘ï¼‰
       - é»˜è®¤ gpt-4o-mini ($0.15/$0.60 per 1M tokens) æä¾›æœ€ä½³é€Ÿåº¦å’Œè´¨é‡å¹³è¡¡
       - å¯é€‰ gpt-5-nano ($0.05/$0.40 per 1M tokens) ä»¥é™ä½æˆæœ¬ï¼ˆä½†å“åº”è¾ƒæ…¢ï¼‰

    ç»§æ‰¿è‡ª BaseTranslationClientï¼Œå·²åŒ…å«ï¼š
    - OutputMixin: ç»Ÿä¸€çš„è¾“å‡ºæ¥å£
    """

    # ç±»å±æ€§ï¼Œç”¨äºè¯†åˆ« provider
    provider = TranslationProvider.OPENAI

    # OpenAI Realtime API ä½¿ç”¨ 24kHzï¼ˆè¾“å…¥å’Œè¾“å‡ºï¼‰
    AUDIO_RATE = 24000

    # æ”¯æŒçš„éŸ³è‰²åˆ—è¡¨
    SUPPORTED_VOICES = {
        "alloy": "Alloy (ä¸­æ€§)",
        "ash": "Ash (ç”·å£°)",
        "ballad": "Ballad (ç”·å£°)",
        "cedar": "Cedar (ä¸­æ€§) â­ æ¨è",
        "coral": "Coral (å¥³å£°)",
        "echo": "Echo (ç”·å£°)",
        "marin": "Marin (ä¸­æ€§) â­ æ¨è",
        "sage": "Sage (å¥³å£°)",
        "shimmer": "Shimmer (å¥³å£°)",
        "verse": "Verse (ç”·å£°)"
    }

    def __init__(
        self,
        api_key: str,
        source_language: str = "zh",
        target_language: str = "en",
        voice: Optional[str] = "marin",
        audio_enabled: bool = True,
        model: str = "gpt-4o-mini-realtime-preview",
        transcribe_model: str = "gpt-4o-mini-transcribe-2025-12-15",
        translation_model: str = "gpt-4o-mini",
        **kwargs
    ):
        """
        åˆå§‹åŒ– OpenAI ç¿»è¯‘å®¢æˆ·ç«¯

        Args:
            api_key: OpenAI API Key
            source_language: æºè¯­è¨€ (zh/en/ja/ko/...)
            target_language: ç›®æ ‡è¯­è¨€ (en/zh/ja/ko/...)
            voice: éŸ³è‰²é€‰æ‹© (ä»… S2S æ¨¡å¼)
            audio_enabled: æ˜¯å¦å¯ç”¨éŸ³é¢‘è¾“å‡ºï¼ˆTrue=S2S, False=S2Tï¼‰
            model: S2S æ¨¡å¼çš„ Realtime æ¨¡å‹åç§°
            transcribe_model: S2T æ¨¡å¼çš„è½¬å½•æ¨¡å‹ (gpt-4o-mini-transcribe-2025-12-15 / gpt-4o-transcribe / gpt-4o-mini-transcribe)
            translation_model: S2T æ¨¡å¼çš„ç¿»è¯‘æ¨¡å‹ (gpt-4o-mini / gpt-5-nano / gpt-5-mini / gpt-4o)
        """
        if not api_key:
            raise ValueError("API key cannot be empty.")

        # OpenAI ç‰¹å®šé…ç½®
        self.model = model
        self.transcribe_model = transcribe_model
        self.translation_model = translation_model
        self.ws = None

        # éŸ³é¢‘é…ç½®ï¼ˆOpenAI ä½¿ç”¨ 24kHz PCM16ï¼‰
        self._input_rate = self.AUDIO_RATE
        self._input_chunk = 2400  # 100ms @ 24kHz
        self._input_format = pyaudio.paInt16
        self._input_channels = 1

        # è°ƒç”¨çˆ¶ç±» __init__
        super().__init__(
            api_key=api_key,
            source_language=source_language,
            target_language=target_language,
            voice=voice,
            audio_enabled=audio_enabled,
            **kwargs
        )

        # S2S è¾“å‡ºé—¨æ§
        self._s2s_expect_response = False
        self._s2s_has_user_audio = False
        self._s2s_speech_rms_threshold = 500

        # S2T æ¨¡å¼ï¼šOpenAI SDK å®¢æˆ·ç«¯ç”¨äºç¿»è¯‘
        self._openai_client = None
        if not audio_enabled and OPENAI_SDK_AVAILABLE:
            self._openai_client = OpenAI(api_key=api_key)

        # S2T: ä¸Šä¸‹æ–‡è¿½è¸ª
        self._previous_transcription = ""
        self._previous_translation = ""

        # S2T: è¯­éŸ³æ´»åŠ¨çŠ¶æ€è¿½è¸ª
        self._speech_active = False  # æ˜¯å¦æ­£åœ¨è¯´è¯ï¼ˆspeech_startedåˆ°speech_stoppedä¹‹é—´ï¼‰
        self._last_output_time = 0.0  # ä¸Šæ¬¡è¾“å‡ºæ—¶é—´ï¼ˆç”¨äºæ˜¾ç¤ºListeningæç¤ºï¼‰
        self._listening_indicator_task = None  # å»¶è¿Ÿæ˜¾ç¤ºListeningçš„ä»»åŠ¡

        # S2T: Delta å¢é‡è½¬å½•è¿½è¸ªï¼ˆç”¨äºæ¸è¿›å¼æ˜¾ç¤ºï¼‰
        self._current_item_id = None
        self._current_delta_transcript = ""
        self._translated_sentences = []  # å·²ç¿»è¯‘çš„å¥å­åˆ—è¡¨ [(en, zh), ...]
        self._last_sentence_count = 0  # ä¸Šæ¬¡å¤„ç†çš„å¥å­æ•°é‡
        # æ˜¾ç¤ºèŠ‚æµï¼ˆæ¯50msæˆ–2ä¸ªæ–°è¯æ›´æ–°æ˜¾ç¤ºï¼‰
        self._last_display_time = 0.0
        self._last_display_word_count = 0
        self._display_throttle_ms = 50  # 50ms - 20 updates/sec max
        self._display_word_delta = 2  # æ¯2ä¸ªæ–°è¯æ›´æ–°ä¸€æ¬¡æ˜¾ç¤º
        # æœªå®Œæˆå¥å­çš„ç¿»è¯‘èŠ‚æµï¼ˆæ¯1ç§’æˆ–4ä¸ªæ–°è¯ï¼‰
        self._last_translation_time = 0.0
        self._last_translation_word_count = 0
        self._translation_throttle_ms = 1000  # 1ç§’
        self._translation_word_delta = 4  # æ¯4ä¸ªæ–°è¯ç¿»è¯‘ä¸€æ¬¡
        self._translation_task = None  # åå°ç¿»è¯‘ä»»åŠ¡
        self._pending_sentence = ""  # æœªå®Œæˆçš„å¥å­ï¼ˆä¸ä»¥.!?ç»“å°¾ï¼‰
        self._pending_translation = ""  # æœªå®Œæˆå¥å­çš„ç¿»è¯‘
        self._last_output_text = ""  # ä¸Šæ¬¡è¾“å‡ºçš„è‹±æ–‡æ–‡æœ¬ï¼ˆç”¨äºå»é‡ï¼‰

        # è¯­è¨€åç§°æ˜ å°„
        self.lang_names = {
            "en": "English",
            "zh": "Chinese",
            "ja": "Japanese",
            "ko": "Korean",
            "fr": "French",
            "de": "German",
            "es": "Spanish"
        }

    @property
    def input_rate(self) -> int:
        """è¾“å…¥é‡‡æ ·ç‡ï¼ˆéº¦å…‹é£ï¼‰"""
        return self._input_rate

    @property
    def output_rate(self) -> int:
        """è¾“å‡ºé‡‡æ ·ç‡ï¼ˆä»… S2Sï¼‰"""
        return self.AUDIO_RATE

    @classmethod
    def get_supported_voices(cls) -> Dict[str, str]:
        """è·å–æ”¯æŒçš„éŸ³è‰²åˆ—è¡¨"""
        return cls.SUPPORTED_VOICES.copy()

    def _get_api_url(self) -> str:
        """æ ¹æ®æ¨¡å¼è¿”å›ä¸åŒçš„ API URL"""
        if self.audio_enabled:
            # S2S: ä½¿ç”¨ä¼šè¯æ¨¡å¼
            return f"wss://api.openai.com/v1/realtime?model={self.model}"
        else:
            # S2T: ä½¿ç”¨çº¯è½¬å½•æ¨¡å¼
            return "wss://api.openai.com/v1/realtime?intent=transcription"

    async def connect(self):
        """å»ºç«‹ WebSocket è¿æ¥"""
        headers = {
            "Authorization": f"Bearer {self.api_key}",
            "OpenAI-Beta": "realtime=v1"
        }

        api_url = self._get_api_url()

        try:
            # æ£€æŸ¥ä»£ç†é…ç½®
            proxy_url = (os.getenv("HTTP_PROXY") or
                        os.getenv("http_proxy") or
                        os.getenv("GLOBAL_AGENT_HTTP_PROXY"))

            if PROXY_AVAILABLE and proxy_url:
                self.output_debug(f"ä½¿ç”¨ä»£ç†: {proxy_url}")
                try:
                    proxy = Proxy.from_url(proxy_url)
                    sock = await proxy.connect(
                        dest_host="api.openai.com",
                        dest_port=443,
                        timeout=10
                    )
                    self.ws = await websockets.connect(
                        api_url,
                        extra_headers=headers,
                        sock=sock,
                        server_hostname="api.openai.com"
                    )
                except Exception as proxy_error:
                    self.output_warning(f"ä»£ç†è¿æ¥å¤±è´¥: {proxy_error}ï¼Œå°è¯•ç›´è¿...")
                    self.ws = await websockets.connect(
                        api_url,
                        extra_headers=headers
                    )
            else:
                self.ws = await websockets.connect(
                    api_url,
                    extra_headers=headers
                )

            self.is_connected = True
            mode = "S2S" if self.audio_enabled else "S2T (streaming transcription)"
            self.output_status(f"å·²è¿æ¥åˆ° OpenAI Realtime API ({mode})")

            # For S2S, configure immediately; for S2T, wait for transcription_session.created
            if self.audio_enabled:
                await self._configure_s2s_session()
        except Exception as e:
            self.output_error(f"è¿æ¥å¤±è´¥: {e}", exc_info=True)
            self.is_connected = False
            raise

    async def configure_session(self):
        """é…ç½®ä¼šè¯ - S2Sç«‹å³é…ç½®ï¼ŒS2Tç­‰å¾…æœåŠ¡å™¨åˆ›å»ºä¼šè¯åé…ç½®"""
        # This method is called from connect() for S2S
        # For S2T, it's called when transcription_session.created event is received
        if self.audio_enabled:
            await self._configure_s2s_session()
        else:
            await self._configure_s2t_session()

    async def _configure_s2t_session(self):
        """é…ç½® S2T ä¼šè¯ - ä½¿ç”¨çº¯è½¬å½•æ¨¡å¼"""
        # Map language codes to what OpenAI expects
        lang = self.source_language
        if lang == "zh":
            lang = "zh"  # Chinese
        elif lang == "en":
            lang = "en"  # English

        # Build transcription prompt with technical context
        # TODO: Move technical terms to external config (e.g., .env or glossary.json)
        #       to avoid hardcoding them in the code
        transcription_config = {
            "model": self.transcribe_model,
            "language": lang,
        }

        # Add prompt for better technical term recognition
        if lang == "en":
            transcription_config["prompt"] = (
                "Technical terms: AI agent, Python, Jupyter notebook, CLI, API, "
                "dev server, VPN, cluster, customer profile, JSON, YAML. "
            )

        # For transcription intent: use transcription_session.update with session wrapper
        config = {
            "type": "transcription_session.update",
            "session": {
                "input_audio_format": "pcm16",
                "input_audio_transcription": transcription_config,
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 500,  # Increased to capture more context
                    "silence_duration_ms": 700,  # Increased to avoid cutting off mid-sentence
                },
                "input_audio_noise_reduction": {
                    "type": "near_field"
                }
            }
        }

        await self.ws.send(json.dumps(config))
        self.output_status(f"S2T ä¼šè¯å·²é…ç½®: {self.transcribe_model} + {self.translation_model}")

    async def _configure_s2s_session(self):
        """é…ç½® S2S ä¼šè¯ - ä½¿ç”¨ä¼šè¯æ¨¡å¼"""
        instructions = self._build_s2s_instructions()

        config = {
            "type": "session.update",
            "session": {
                "modalities": ["text", "audio"],
                "instructions": instructions,
                "voice": self.voice,
                "input_audio_format": "pcm16",
                "output_audio_format": "pcm16",
                "input_audio_transcription": {
                    "model": "whisper-1"
                },
                "turn_detection": {
                    "type": "server_vad",
                    "threshold": 0.5,
                    "prefix_padding_ms": 300,
                    "silence_duration_ms": 300
                },
                "temperature": 0.6,
                "max_response_output_tokens": 4096
            }
        }

        await self.ws.send(json.dumps(config))

    def _build_s2s_instructions(self) -> str:
        """æ„å»º S2S æ¨¡å¼çš„æŒ‡ä»¤"""
        source = self.lang_names.get(self.source_language, self.source_language)
        target = self.lang_names.get(self.target_language, self.target_language)

        instructions = f"""You are a real-time interpreter. Translate {source} speech to {target}.
Output ONLY the translation. No greetings, no confirmations, no questions.
If silence, output nothing."""

        if self.glossary:
            terms = ", ".join([f"{s}={t}" for s, t in list(self.glossary.items())[:20]])
            instructions += f"\nTerminology: {terms}"

        return instructions

    def _translate_text(self, text: str) -> str:
        """
        ä½¿ç”¨ GPT ç¿»è¯‘æ–‡æœ¬ï¼ˆS2T æ¨¡å¼ï¼‰

        Args:
            text: æºè¯­è¨€æ–‡æœ¬

        Returns:
            ç¿»è¯‘åçš„æ–‡æœ¬
        """
        if not self._openai_client:
            self.output_warning("OpenAI SDK ä¸å¯ç”¨ï¼Œè·³è¿‡ç¿»è¯‘")
            return ""

        try:
            source_lang = self.lang_names.get(self.source_language, self.source_language)
            target_lang = self.lang_names.get(self.target_language, self.target_language)

            system_content = f"""You are a professional translator. Translate {source_lang} to {target_lang}.

Rules:
- Output ONLY the translation, nothing else
- Preserve technical terms and proper nouns
- Maintain natural, fluent {target_lang}
- Do not add explanations or notes"""

            # æ·»åŠ ä¸Šä¸‹æ–‡ä»¥æé«˜è¿è´¯æ€§
            if self._previous_transcription:
                system_content += f"""

Previous context:
"{self._previous_transcription}"
Use this for continuity."""

            # GPT-5/reasoning models use max_completion_tokens, older models use max_tokens
            completion_params = {
                "model": self.translation_model,
                "messages": [
                    {"role": "system", "content": system_content},
                    {"role": "user", "content": text}
                ],
            }

            # GPT-5 family (gpt-5*) and reasoning models (o1, o3, o4) use max_completion_tokens
            # and don't support temperature parameter (only default temperature=1)
            if self.translation_model.startswith(("gpt-5", "o1", "o3", "o4")):
                completion_params["max_completion_tokens"] = 1000
            else:
                completion_params["max_tokens"] = 1000
                completion_params["temperature"] = 0.3

            response = self._openai_client.chat.completions.create(**completion_params)

            result = response.choices[0].message.content
            return result.strip() if result else ""

        except Exception as e:
            self.output_error(f"ç¿»è¯‘å¤±è´¥: {e}")
            return ""

    async def send_audio_chunk(self, audio_data: bytes):
        """å‘é€éŸ³é¢‘æ•°æ®å—"""
        if not self.is_connected or not self.ws:
            return

        try:
            if self.audio_enabled:
                try:
                    if audioop.rms(audio_data, 2) > self._s2s_speech_rms_threshold:
                        self._s2s_has_user_audio = True
                except Exception:
                    pass

            event = {
                "type": "input_audio_buffer.append",
                "audio": base64.b64encode(audio_data).decode()
            }
            await self.ws.send(json.dumps(event))
        except Exception as e:
            self.output_error(f"å‘é€éŸ³é¢‘å—å¤±è´¥: {e}")
            self.is_connected = False

    async def handle_server_messages(self, on_text_received=None):
        """å¤„ç†æœåŠ¡å™¨æ¶ˆæ¯"""
        try:
            async for message in self.ws:
                try:
                    event = json.loads(message)
                    event_type = event.get("type")

                    # ======== å…±äº«äº‹ä»¶ ========
                    if event_type == "session.created" or event_type == "session.updated":
                        pass

                    elif event_type == "transcription_session.created":
                        # S2T: è½¬å½•ä¼šè¯å·²åˆ›å»ºï¼Œç°åœ¨å‘é€é…ç½®
                        self.output_status("Transcription session created, configuring...")
                        await self._configure_s2t_session()

                    elif event_type == "transcription_session.updated":
                        pass

                    elif event_type == "input_audio_buffer.speech_started":
                        self._speech_active = True
                        # Start delayed task to show "Listening..." if no output after 3s
                        self._cancel_listening_indicator()
                        self._listening_indicator_task = asyncio.create_task(
                            self._show_listening_indicator_after_delay(3.0)
                        )

                    elif event_type == "input_audio_buffer.speech_stopped":
                        self._speech_active = False
                        self._cancel_listening_indicator()
                        self._s2s_expect_response = self.audio_enabled and self._s2s_has_user_audio
                        self._s2s_has_user_audio = False

                    elif event_type == "input_audio_buffer.committed":
                        pass

                    # ======== S2T Transcription Events ========
                    elif event_type == "conversation.item.created":
                        item_id = event.get("item", {}).get("id", "")
                        if item_id:
                            self._reset_transcription_state(item_id)

                    elif event_type == "conversation.item.input_audio_transcription.delta":
                        item_id = event.get("item_id", "")
                        delta = event.get("delta", "")
                        if item_id == self._current_item_id and delta:
                            self._current_delta_transcript += delta
                            await self._handle_s2t_delta(self._current_delta_transcript)

                    elif event_type == "conversation.item.input_audio_transcription.completed":
                        item_id = event.get("item_id", "")
                        transcript = event.get("transcript", "").strip()
                        if item_id == self._current_item_id and transcript:
                            self._cancel_pending_translation()
                            self._current_delta_transcript = ""
                            await self._handle_s2t_transcription(transcript)

                    # ======== S2S Conversation Events ========
                    elif event_type == "response.audio.delta" and self.audio_enabled:
                        if not self._s2s_expect_response:
                            continue
                        audio_b64 = event.get("delta", "")
                        if audio_b64:
                            audio_data = base64.b64decode(audio_b64)
                            self._queue_audio(audio_data)

                    elif event_type == "response.audio_transcript.done":
                        if not self._s2s_expect_response:
                            continue
                        transcript = event.get("transcript", "")
                        self.output_translation(transcript, extra_metadata={"provider": "openai", "mode": "S2S"})

                    elif event_type == "response.done":
                        self._s2s_expect_response = False

                    # ======== é”™è¯¯å¤„ç† ========
                    elif event_type == "error":
                        error = event.get("error", {})
                        error_code = error.get("code", "Unknown")
                        error_msg = error.get("message", "Unknown error")
                        self.output_error(f"{error_code}: {error_msg}")

                        if "connection" in error_code.lower() or "unauthorized" in error_code.lower():
                            self.is_connected = False
                            break

                except json.JSONDecodeError as e:
                    self.output_warning(f"è§£ææ¶ˆæ¯å¤±è´¥: {e}")
                    continue
                except Exception as e:
                    self.output_warning(f"å¤„ç†äº‹ä»¶æ—¶å‡ºé”™: {e}")
                    continue

        except websockets.exceptions.ConnectionClosed:
            self.output_warning("WebSocket è¿æ¥å·²å…³é—­")
            self.is_connected = False
        except Exception as e:
            self.output_error(f"æ¶ˆæ¯å¤„ç†é”™è¯¯: {e}", exc_info=True)
            self.is_connected = False

    async def _show_listening_indicator_after_delay(self, delay_seconds: float):
        """Show 'Listening...' indicator if no output after delay while speech is active."""
        try:
            await asyncio.sleep(delay_seconds)
            time_since_output_ms = time.time() * 1000 - self._last_output_time
            if self._speech_active and time_since_output_ms >= 3000:
                self.output_subtitle(
                    target_text="...",
                    source_text="ğŸ¤ Listening...",
                    is_final=False,
                    extra_metadata={"provider": "openai", "mode": "S2T", "stage": "Listening"}
                )
        except asyncio.CancelledError:
            pass

    async def _handle_s2t_delta(self, partial_transcript: str):
        """Process incremental S2T transcription (delta events).

        Splits text into sentences at punctuation marks (.!?,ã€‚ï¼ï¼Ÿï¼Œ).
        Complete sentences are translated immediately.
        Incomplete sentences are shown in source language until complete.
        """
        if not partial_transcript or not partial_transcript.strip():
            return

        text = partial_transcript.strip()

        # Split into sentences at punctuation boundaries
        sentence_pattern = r'([.!?,ã€‚ï¼ï¼Ÿï¼Œ]+)'
        parts = re.split(sentence_pattern, text)

        # Reassemble: pair each text segment with its trailing punctuation
        sentences = []
        for i in range(0, len(parts) - 1, 2):
            segment = parts[i].strip()
            if segment:
                punctuation = parts[i + 1] if i + 1 < len(parts) else ""
                sentences.append(segment + punctuation)

        # Text after the last punctuation is the incomplete "pending" portion
        pending = parts[-1].strip() if len(parts) % 2 == 1 else ""

        # Translate any new complete sentences (sequentially for proper timestamps)
        if len(sentences) > self._last_sentence_count:
            # Cancel any pending translation task to avoid duplicate output
            if self._translation_task and not self._translation_task.done():
                self._translation_task.cancel()
                self._translation_task = None

            for sentence in sentences[self._last_sentence_count:]:
                await self._translate_and_output_sentence(sentence, is_final=True)
            self._last_sentence_count = len(sentences)
            self._pending_sentence = ""
            self._pending_translation = ""

        # Handle incomplete sentence at the end
        if pending and pending != self._pending_sentence:
            self._pending_sentence = pending
            word_count = len(pending.split())
            time_since_last_translation = time.time() * 1000 - self._last_translation_time

            # Translate if long enough (4+ words) and throttle time passed (800ms)
            if word_count >= 4 and time_since_last_translation >= 800:
                self._last_translation_time = time.time() * 1000
                # Store task reference so it can be cancelled if complete sentence arrives
                self._translation_task = asyncio.create_task(
                    self._translate_and_output_sentence(pending, is_final=False)
                )
            else:
                # Show source text without translation
                self.output_subtitle(
                    target_text="",
                    source_text=pending,
                    is_final=False,
                    extra_metadata={"provider": "openai", "mode": "S2T", "stage": "Pending"}
                )

    async def _translate_and_output_sentence(self, sentence: str, is_final: bool = True):
        """Translate a sentence and output it.

        Args:
            sentence: The sentence to translate
            is_final: If True, adds to history. If False, shows as temporary preview.
        """
        try:
            normalized = self._normalize_text(sentence)
            last_normalized = self._normalize_text(self._last_output_text)

            # Skip non-final outputs if:
            # 1. Same text already output, OR
            # 2. This is a prefix of what was already output (longer version shown)
            if not is_final and normalized and last_normalized:
                if normalized == last_normalized or last_normalized.startswith(normalized):
                    return

            loop = asyncio.get_event_loop()
            translation = await loop.run_in_executor(None, self._translate_text, sentence)

            if translation:
                self.output_subtitle(
                    target_text=translation,
                    source_text=sentence,
                    is_final=is_final,
                    extra_metadata={"provider": "openai", "mode": "S2T", "stage": "Sentence"}
                )

                self._last_output_time = time.time() * 1000
                self._cancel_listening_indicator()

                self._last_output_text = sentence
                self._previous_transcription = sentence
                self._previous_translation = translation

        except asyncio.CancelledError:
            pass
        except Exception as e:
            self.output_debug(f"Sentence translation failed: {e}")

    @staticmethod
    def _normalize_text(text: str) -> str:
        """Remove punctuation and whitespace for text comparison."""
        return re.sub(r'[.!?,ã€‚ï¼ï¼Ÿï¼Œ\s]+', '', text.lower())

    def _cancel_listening_indicator(self):
        """Cancel any pending listening indicator task."""
        if self._listening_indicator_task:
            self._listening_indicator_task.cancel()
            self._listening_indicator_task = None

    def _cancel_pending_translation(self):
        """Cancel any pending background translation task."""
        if self._translation_task and not self._translation_task.done():
            self._translation_task.cancel()

    def _reset_transcription_state(self, item_id: str):
        """Reset transcription state for a new conversation item."""
        self._current_item_id = item_id
        self._current_delta_transcript = ""
        self._translated_sentences = []
        self._last_sentence_count = 0
        self._pending_sentence = ""
        self._pending_translation = ""
        self._last_display_time = 0.0
        self._last_display_word_count = 0
        self._last_translation_time = 0.0
        self._last_translation_word_count = 0
        self._last_output_text = ""

    async def _handle_s2t_transcription(self, transcript: str):
        """Handle completed S2T transcription (triggered by VAD silence detection).

        Flushes any remaining pending content that wasn't processed by delta handler.
        """
        # Flush pending content (text without punctuation at the end)
        if self._pending_sentence and len(self._pending_sentence.strip()) >= 2:
            await self._translate_and_output_sentence(self._pending_sentence, is_final=True)
            self._pending_sentence = ""
            self._pending_translation = ""

        self._previous_transcription = transcript

    async def close(self):
        """å…³é—­è¿æ¥å¹¶æ¸…ç†èµ„æº"""
        self.output_status("å…³é—­è¿æ¥...")
        self.is_connected = False

        if self.ws:
            try:
                await asyncio.wait_for(self.ws.close(), timeout=2.0)
                self.output_debug("WebSocket å·²å…³é—­")
            except asyncio.TimeoutError:
                self.output_warning("WebSocket å…³é—­è¶…æ—¶")
            except Exception as e:
                self.output_warning(f"å…³é—­ WebSocket æ—¶å‡ºé”™: {e}")

    def generate_voice_sample_file(self, voice: str, text: str = "This is a common phrase used in business meetings."):
        """
        ç”ŸæˆéŸ³è‰²æ ·æœ¬æ–‡ä»¶ï¼ˆOpenAI å®ç°ï¼‰
        """
        from pathlib import Path
        from paths import VOICE_SAMPLES_DIR, ASSETS_DIR

        filename = f"openai_{voice}.wav"
        filepath = VOICE_SAMPLES_DIR / filename

        if filepath.exists():
            return str(filepath)

        standard_audio = ASSETS_DIR / "voice_sample_input_24k.wav"
        if not standard_audio.exists():
            return ""

        async def _generate():
            try:
                original_voice = self.voice
                original_audio_enabled = self.audio_enabled
                self.voice = voice
                self.audio_enabled = True

                await self.connect()

                instructions = self._build_s2s_instructions()
                sample_config = {
                    "type": "session.update",
                    "session": {
                        "modalities": ["text", "audio"],
                        "instructions": instructions,
                        "voice": self.voice,
                        "input_audio_format": "pcm16",
                        "output_audio_format": "pcm16",
                        "turn_detection": {
                            "type": "server_vad",
                            "threshold": 0.5,
                            "prefix_padding_ms": 300,
                            "silence_duration_ms": 2000
                        },
                        "temperature": 0.8,
                        "max_response_output_tokens": 4096
                    }
                }
                await self.ws.send(json.dumps(sample_config))

                with open(standard_audio, 'rb') as f:
                    f.seek(44)
                    audio_data = f.read()

                audio_chunks = []
                response_complete = False

                async def collect_messages():
                    nonlocal audio_chunks, response_complete
                    try:
                        async for message in self.ws:
                            try:
                                event = json.loads(message)
                                event_type = event.get("type", "")

                                if event_type == "response.audio.delta" and self.audio_enabled:
                                    audio_b64 = event.get("delta", "")
                                    if audio_b64:
                                        chunk_data = base64.b64decode(audio_b64)
                                        audio_chunks.append(chunk_data)

                                elif event_type == "response.done":
                                    response_complete = True
                                    break

                                elif event_type == "error":
                                    break

                            except json.JSONDecodeError:
                                continue
                            except Exception:
                                continue

                            if response_complete:
                                break

                    except Exception:
                        pass

                message_task = asyncio.create_task(collect_messages())
                await asyncio.sleep(0.5)

                chunk_size = 100 * 1024
                chunk_count = 0
                for i in range(0, len(audio_data), chunk_size):
                    chunk = audio_data[i:i + chunk_size]
                    await self.send_audio_chunk(chunk)
                    chunk_count += 1
                    if chunk_count % 3 == 0 and i + chunk_size < len(audio_data):
                        await asyncio.sleep(0.1)

                import struct
                silence_duration = 2.0
                silence_samples = int(self.output_rate * silence_duration)
                silence_data = struct.pack('<' + 'h' * silence_samples, *[0] * silence_samples)

                silence_chunk_size = 100 * 1024
                for i in range(0, len(silence_data), silence_chunk_size):
                    chunk = silence_data[i:i + silence_chunk_size]
                    await self.send_audio_chunk(chunk)

                try:
                    await asyncio.wait_for(message_task, timeout=30.0)
                except asyncio.TimeoutError:
                    pass

                if audio_chunks:
                    full_audio = b''.join(audio_chunks)

                    import wave
                    filepath.parent.mkdir(parents=True, exist_ok=True)
                    with wave.open(str(filepath), 'wb') as wf:
                        wf.setnchannels(1)
                        wf.setsampwidth(2)
                        wf.setframerate(self.output_rate)
                        wf.writeframes(full_audio)

                    return str(filepath)
                else:
                    return ""

            except Exception:
                return ""
            finally:
                self.voice = original_voice
                self.audio_enabled = original_audio_enabled
                try:
                    await self.close()
                except:
                    pass

        try:
            import concurrent.futures
            with concurrent.futures.ThreadPoolExecutor() as executor:
                future = executor.submit(asyncio.run, _generate())
                return future.result(timeout=40)
        except Exception:
            return ""
